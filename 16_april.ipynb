{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869b99fb-d629-4fb2-8d09-5fbad268ebeb",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc937e-aec1-4756-97f3-866dd38efa3b",
   "metadata": {},
   "source": [
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d6e7e-4d38-460f-a0cb-636e18e576f4",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856accf4-0a05-4e2e-a66b-581d04a0643b",
   "metadata": {},
   "source": [
    "Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c6e9f-5a96-4383-845f-a759d74db635",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd70165-cb62-4457-bfdb-9afe46453476",
   "metadata": {},
   "source": [
    "Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d312b-75e4-4a7d-8b35-bd456c496d19",
   "metadata": {},
   "source": [
    "Types of Boosting Algorithms:-\n",
    "\n",
    "It can be decision stamp, margin-maximizing classification algorithm etc. There are many boosting algorithms which use other types of engine such as: AdaBoost (Adaptive Boosting) Gradient Tree Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc17a34-2320-42e0-9d31-31f6267bc9ef",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be050a50-0660-43a1-9213-79c771afbe10",
   "metadata": {},
   "source": [
    "To mitigate this problem, gradient boosting has a parameter called learning rate. The learning rate in gradient boosting is simply a multiplier between 0 and 1 that scales the prediction of each weak learner (see the section below for details on learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15fb2f-0990-4cda-9f00-de144f5e7160",
   "metadata": {},
   "source": [
    "Q6.How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426f54f-e29d-4fe8-bfdc-4a4ef41db400",
   "metadata": {},
   "source": [
    "In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor. With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a286b2-b737-4307-a7f8-76ab7ba1eadb",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc833-af00-4cc9-91d0-c0adfcc4f91b",
   "metadata": {},
   "source": [
    "AdaBoost is a boosting algorithm that creates a strong classifier by combining weak classifiers. A weak classifier is simply a classifier that performs poorly, but better than random guessing. The algorithm works by weighting the weak classifiers so that they vote with more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0442a8-6dae-4ece-a0e3-b873d7e01938",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05393ae-a0fe-4e31-bd7a-6f11c7600618",
   "metadata": {},
   "source": [
    "For AdaBoost the basis functions are the classifiers Gm, and they produce the output of either — 1 or + 1. Using the exponential loss function, we now must solve at every step that we essentially minimise over β and G the sum over the exponential loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f429754-f732-4053-ab80-7bd10f69ae7a",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdf377-8d04-47ad-82a5-af9ecf72843f",
   "metadata": {},
   "source": [
    "AdaBoost combines multiple weak classifiers to create a strong classifier, with higher weights assigned to misclassified data in each iteration, boosting their significance in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5522c6f-f972-4f13-8532-bbab58f5a0cb",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1483ad-ec51-42a3-a196-9a196ca585f5",
   "metadata": {},
   "source": [
    "The effect of increasing the number of estimators in AdaBoost algorithm:-\n",
    "\n",
    "1.There are primarily three hyperparameters that you can tune to improve the performance of AdaBoost: The number or estimators, learning rate and maximum number of splits.\n",
    "\n",
    "2.It's really hard to give general guidelines for optimal values for these parameters, as it always depends on the problem and the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
